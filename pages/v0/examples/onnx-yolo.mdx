# ONNX Runtime - Object detection with YOLO

import { Steps, Callout } from 'nextra/components';

This example makes use of the built-in ONNX Runtime to run object detection using a YOLO model.

After the inference, we post-process the results and use the `draw` plugin to draw the bounding boxes.

The ONNX model to use is included within the example code. FYI it was obtained by simply exporting the YOLOv8n model to the ONNX format, so it can be loaded into the ONNX Runtime.

<Callout>
Please note that even if this examples uses a YOLO model, we are NOT using the YOLO sdk here. We simply load the YOLO model in ONNX format from a file in the same way you can load any ONNX model.
</Callout>

You can directly execute the example following the steps below or learn what each line of the application does in the [walkthrough section](#walkthrough).

You can check the whole documentation of the draw plugin at:

* [Drawing plugin](https://pipeless.ai/docs/v0/plugins/draw)

## Requirements

* Pipeless: `pip install pipeless_ai[onnx-cpu] pipeless_ai_cli`. this command will install ONNX for CPU inference. If you have CUDA or TensorRT and the dependencies install you can change `cpu` by `gpu`.

* Pipeless Drawing plugin: `pipeless install plugin draw`

<Callout>
    If you don't have the required dependencies for Pipeless installed check the [installation guide](/v0/install) or [use Docker](/v0/install#using-docker-) instead.
</Callout>

## Run the example

<Steps>
### Clone the repo

```bash copy
git clone https://github.com/miguelaeh/pipeless
cd pipeless
```

### Move to the `examples/onnx-yolo` directory

``` bash copy
cd examples/onnx-yolo
```

### Configure your input and output (optional)

Update the following paths on `config.yaml` to match your system paths:

```yaml {7,15,21} filename="config.yaml"
input:
  address:
    host: localhost
    port: 1234
  video:
    enable: true
    uri: 'file:///home/path/pipeless/examples/onnx-yolo/input.mp4'
log_level: INFO
output:
  address:
    host: localhost
    port: 1237
  video:
    enable: true
    uri: 'file:///home/path/pipeless/examples/onnx-yolo/output.mp4'
plugins:
  order: 'draw'
worker:
  n_workers: 1
  inference:
    model_uri: 'file:///home/path/pipeless/examples/onnx-yolo/yolov8n.onnx'
    image_shape_format: 'channels, height, width'
```

### Run the application

Simply execute:

```bash copy
pipeless run
```

### Visualize the output

You can use any video player to see the results. For example:

```bash copy
mpv output.mp4
```

</Steps>


## Walkthrough

This is a step by step description of what every line of the application code is doing.

In this case, there is no `process` hook, since our processing will be automatically done by the ONNX Runtime and corresponds to run inference using the provided model.

### Pre-process hook

Even if we are using the ONNX Runtime and Pipeless performs some automatic pre-processing to match the model input format, there are still some points that Pipeless can't infer.
In this case, our model expects a normalized input image, thus, we will perform normalization in the `pre-process` hook.

```python copy
def pre_process(self, frame):
    frame = cv2.normalize(frame, None, 0.0, 1.0, cv2.NORM_MINMAX)
    return  frame
```

The normalization is a simple process to convert the image RGB pixels into values in the range 0 to 1. The code above is more or less equivalent to divide the frame RGB valus by `255`.

### Post-process hook

Since we are using the ONNX Runtime, the frame provided in the `post-process` hook parameters is the original frame (check the [ONNX docs page](/v0/inference) to learn why).

We can access the inference results at `self.inference.results`, which is the output of the inference session, or what is the same, what our model returns.
In this case, since we are performing object detection, it will be an array of bounding boxes with their respective scores and labels. The rest of the code within this hook takes care of converting that output into something we can understand as follows:

1. Extract the prediction rows that have a score over a hardcoded threshold, commonly called the confidence threshold.
1. Extract the corresponding class ID of each of the predictions obtained in the previous step.
1. Extract the bounding boxes values (x,y,w,h) from the prediction.
1. Escale the boxes to the original image size. This is very important because the bounding boxes will correspond to the model input image, which is almost always resized to match the model input format.
1. Convert the bounding boxes from x,y,w,h, to x,y,x,y. We perform this step simply to allow th draw plugin to draw them for us.
1. Perform non max suppresion. This is a common step in post-processing and consists on the elimination of redundant bounding boxes. In this case, we use a method from OpenCV that takes care of it.
1. Provide our bounding boxes, scores, labels and the color in which they will be drawn to the drawing plugin.

The following is the function that takes care of most of the steps above, except from providing the values to the drawing plugin:

```python copy
def process_output(model_output, orginal_image_shape, model_input_shape):
    confidence_threshold = 0.1
    iou_threshold = 0.5

    predictions = np.squeeze(model_output[0]).T

    scores = np.max(predictions[:, 4:], axis=1)
    predictions = predictions[scores > confidence_threshold, :]
    scores = scores[scores > confidence_threshold]
    if len(scores) == 0:
        return [], [], []

    class_ids = np.argmax(predictions[:, 4:], axis=1)

    # Extract boxes
    boxes = predictions[:, :4]
    boxes = rescale_boxes(orginal_image_shape, model_input_shape, boxes)
    boxes = xywh2xyxy(boxes)

    indices = cv2.dnn.NMSBoxes(boxes, scores, confidence_threshold, iou_threshold)
    return boxes[indices], scores[indices], class_ids[indices]
```

And the following is our Pipeless `post-process` hook:

```python copy
def post_process(self, frame):
    model_output = self.inference.results
    yolo_input_shape = (640, 640, 3)
    boxes, scores, class_ids = process_output(model_output, frame.shape, yolo_input_shape)
    assert len(boxes) == len(scores) == len(class_ids), "Boxes, scores and class_ids must have the same length"
    class_labels = [yolo_classes[id] for id in class_ids]
    color = (0, 255, 255)
    for i in range(len(boxes)):
        self.plugins.draw.boxes.append([*boxes[i], class_labels[i], scores[i], color])
    return frame
```

You can check the util functions used on the `app.py` file if you are interested in checking deeper the code of each of the steps.
