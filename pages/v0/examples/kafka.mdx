# Sending messages to Kafka and reacting to events in real time

import { Steps, Callout } from 'nextra/components';

This example uses Pipeless and Kafka and demonstrates how to send data to a Kafka topic to which you can subscribe to perform actions by receiving events in real time.

In this case, we are not interested in the output video since we just want to analyze the input and create events based on the elements that appear on it, so **we will disable the video output**.

We will do the same detections that we did on the [cats](../cats/) example, but instead of drawing the bounding box around the cat face, we will send a message to a Kafka topic to get notified that there is a cat on the video.

To connect to Kafka we will use the **Pipeless Kafka Plugin**. This plugin is a wrapper around the `confluent_kafka` package for more convenient usage, but you could modify the example to use any Kafka client directly from your hooks implementation or even replace Kafka with any event streaming platform or messaging broker. Check the whole documentation of the [Pipeless Kafka Plugin](/v0/plugins/kafka).

For simplicity, we have included a Kafka `docker-compose.yaml` on this same directory so you can easily start a local Kafka cluster for testing.

You can directly execute the example. To learn what each line of the application does on the [walkthrough section](#walkthrough).

## Requirements

* Pipeless: `pip install pipeless_ai_cli`

* OpenCV: `pip install opencv-python`

<Callout>
    If you don't have the required dependencie for Pipeless installed check the [installation guide](/v0/install) or [use Docker](/v0/install#using-docker-) instead.
</Callout>

## Architecture

The following is the architecture of this example:

![Pipeless + Kafka schema](https://pipeless.ai/kafka-white-bg.jpg)

## Run the example

<Steps>

### Clone the repo

```bash copy
git clone https://github.com/pipeless-ai/pipeless
cd pipeless
```

### Move to the `examples/kafka` directory

``` bash copy
cd examples/kafka
```

### Configure your paths

Update `config.yaml` with the paths used within your system. They **must be absolute paths**. You need to edit the lines highlighted below:

```yaml {7}filename="config.yaml"
input:
  address:
    host: localhost
    port: 1234
  video:
    enable: true
    uri: file:///home/path/pipeless/examples/kafka/cats.mp4
log_level: INFO
output:
  video:
    enable: false
worker:
  n_workers: 1
plugins:
  order: 'kafka'
```

### Install the Pipeless Kafka plugin

Run the following command to install the Pipeless Kafka plugin:

```bash copy
pipeless install plugin kafka
```

### Deploy Kafka

To use the `docker-compose.yaml` file included to deploy a local Kafka instance run:

```bash copy
docker compose up
```

You can verify it is started by running `docker compose logs kafka`. You should see something like the following indicating the Kafka server has started properly:

```
kafka-kafka-1  | [2023-09-02 14:38:36,823] INFO [KafkaRaftServer nodeId=0] Kafka Server started (kafka.server.KafkaRaftServer)
```

### Configure Kafka Plugin

Simply export the following env var. If you used the provided `docker-compose.yaml`, just copy and paste the command below, if not, you need to edit the address:

```bash copy
export KAFKA_BOOTSTRAP_SERVERS="localhost:9094"
```

### Run the application

Simply execute:

```bash copy
pipeless run
```
</Steps>

## Consume the data that was sent to the Kafka topic

The commands in this section must be executed within the Kafka container. Exec into the container by running:

```bash copy
docker compose exec kafka bash
```

The `docker-compose.yaml` included on configured Kafka to automatically create topics. You can verify the `pipeless` topic was created when writing to it:

```bash copy
kafka-topics.sh  --list --bootstrap-server localhost:9094
```

The code of the example only sends information to Kafka, it does not consume from the topic, thus the topic still contains all the information we have sent to it. It is your task to listen for messages on the Kafka topics and take actions based on those messages. Let's run a consumer to verify the information is arriving to the topic:

```bash copy
kafka-bash-consumer.sh --bootstrap-server localhost:9094 --topic pipeless --from-beginning
```

<Callout>Use `Ctrl + C` to stop the consumer. It will not end automatically.</Callout>

And that's all. Now, it is your time to complete your application by listening and consuming messages from the Kafka topic, processing that information and taking any required actions. A simple example could be to send us a notification when a cat appears in the image. In this case that will correspond to identifying a bounding box on a frame since the model we loaded only identifies cats, so we have no labels.

## Walkthrough

This is a step by step description of what every line of the application code is doing. This time, it describes only the new lines related to the Kafka example, to understand how the recognition model is used please check the [cats example walkthrough](/v0/examples/cats/#walkthrough).

### Extra configuration

Since we want to use the Kafka plugin, after installing it, we have to specify to Pipeless the plugins execution order. To do that, the `config.yaml` contains the following lines (we are using only the kafka plugin):

```yaml copy
plugins:
  order: 'kafka'
```

If we had more than one plugin, we would need to specify the comma separated list for the execution order.

### Processing the stream

In this case, instead of editing the frame to draw a bounding box and return the modified video frame like we did on the original cats example, we simply identify the bounding box and, if there is a bounding box, we send a message to the `pipeless` kafka topic:

```python copy
if len(bounding_boxes) > 0:
    self.plugins.kafka.produce('pipeless', 'There is a cat!')
```

The plugin takes care of authenticating with Kafka and we just need to call the `produce` method.

<Callout>The bounding boxes detection is exactly the same as on the cats example.</Callout>
