# Running inference with the ONNX Runtime

The Pipeless worker has the [ONNX Runtime](https://onnxruntime.ai/) embedded. To enable out-of-the-box inference using the ONNX Runtime just provide a model URI in the `worker.inference.model_uri` configuration option. It can be a local URI (starting with `file://`) or a remote URI and it will be automatically downladed (starting with `http://` or `https://`).

The built-in ONNX Runtime is able to run your model either on CPU or GPU. It automatically uses GPU when available and currently supports the CUDA and TensorRT execution providers when using GPU.

When the host does not support the CUDA or TensorRT execution providers, a warning will be shown and the execution will fallback to the next execution provider with higher priority. The priority is `TensorRTExecutionProvider`, `CUDAExecutionProvider` and `CPUExecutionProvider` from highest to lower. 

## Automatic model version updates

Pipeless is able to update your model OpSet (Operation Set) and IR (Intermediate Representation) versions automatically. This is usefull, for example, when chaining a pre-processing model that is on a different version than the main model.

Simply provide `worker.inference.force_ir_version` and `worker.inference.forcer_opset_version` and both models will be automatically converted to the proposed versions.

## Automatic frame pre-processing

Pipeless expects your model to accept an image as input and tries to automatically detect the input format to perform the required transformations. Specifically, it performs the following automatic actions to the input frame:

* Scaling: Pipeless automatically scales the frame to the size expected by the model input.
* Transposition of the frame shape: Pipeless uses a `height, width, channels` shape for frames. If your model accepts a different shape, simply provide it in the `worker.inference.image_shape_format` configuration property as a string, and Pipeless will perform the required change. Note this change only happens internally, which means that in your application hooks the frame received is always on the `height, width, channels` format, independently of what your model accepts.
* Type conversion: The default Pipeless frames contain `uint8` values for the RGB values. It automatically converts the values if required byt your model.
* Add a batch size: It is a common approach to accept batch sizes. Pipeless will add the batch size as the first dimension to the model input if required. Note it always adds it to the first dimension which is the convention used by most of the models. 

Any transformation not included on the list above needs to be done by yourself by either implementing the `pre-process` hook and/or providing a pre-processing model. See the sections below about each of the options.

Check the list of configuration options for the automatic pre-processing in the [configuration options list](/v0/configuration) under the `worker.inference` key.

### Adding a pre-processing model

It is a common practise to pass the input frame through a pre-processing model before running inference on the actual model. Simply provide your pre-processing model URI in the `worker.inference.pre_process_model_uri` configuration option, and it will be automatically chained with your actual model.

### Adding custom pre-processing code

If you do not have a pre-processing model, you can use Python to implement the `pre-process` hook. The return value of this hook is passed to the automatic transformation process to match your model input and then it is passed as input for the inference. 
You can use a pre-processing model and implement the `pre-process` hook at the same time. In that case, the output of your `pre-process` hook is passed to the transformation process and then to your pre-processing model as input.

Examples of common loginc in the pre-processing is to change from RGB to BGR, to normalize an image, to adapt the image to a mean value, etc. We recommend using OpenCV for these tasks. 

### Forcing input image format

In the cases in which Pipeless is not able to detect the image format correctly from the ONNX model, you can provide the following options so that Pipeless can perform the automatic resizing of the images as expected by the model:

* `worker.inference.image_width`: Indicates the image width expected by the model.
* `worker.inference.image_height`: Indicates the image height expected by the model.
* `worker.inference.image_channels`: Indicates the numer of image channels.

You can provide all of the options above with independence of the others.

## Post-processing inference results

You may need to `post-process` the results of the inference engine. You can access the inference results with `self.inference.results` within the `post-process` hook and perform any required actions with them.

Commonly people use the post-process hook to draw the results over the original frame, export the output to some external service, take some actions via a device API, etc.

In case you are using the ONNX Runtime, the `post-process` hook will receive the original frame as parameter, unlike when not using the ONNX Runtime, when it will receive the frame chained from previous hooks.
When you pre-process the frame to provide it as input to the model, it suffers several changes that are only useful for inference, but, once you have the inference results, you are no longer interested in those changes. Thus, making the `post-process` hook to receive the original frame in this case instead of the `pre-process` hook output makes your life easier. Note when you enable an output video, you need to return a valid frame from the `post-process` hook, based on the inference results and usually modifying the original frame.

You can also access the original frame within any processing hook using `self.original_frame`.

## Troubleshooting

### Seems to get stuck

If running `pipeless run` using the ONNX inference it seems to get stuck and there is not a clear error shown, try running the worker only to see the error with `pipeless run worker`.

### I have CUDA and Cudnn installed but ONNX Runtime is not using CUDA

In this case you usually get a warning like:

```
[W:onnxruntime:Default, onnxruntime_pybind_state.cc:640 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.
```

Ensure you have the proper CUDA and Cudnn versions installed. Check the [ONNX CUDA requirements](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html) to ensure your versions are compatible. We recommend using CUDA version `11.7` since `12.X` is not properly recognized by the latest version of the ONNX Runtime.

Optionally, you can use the Pipeless CUDA container image. Find the docs [here](/v0/container).

### Segmentation fault (core dumped)

The most common error that leads to a segmentation fault is to provide wrong input data to the model. Double check that what your `pre-process` hook is returning follows the model input format (previous to the automatic transformations mentioned above).

Another common reason is a wrong CUDA version for your target GPU.

If you suffer a segmentation fault and you are unable to understand why, open a GitHub issue providing all the information you can, so we can help you figure it out.
